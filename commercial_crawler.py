#!/usr/bin/env python3
"""
ìƒìš©í™” ì›¹ í¬ë¡¤ëŸ¬ - Phase 1: ê¸°ìˆ ì  ì™„ì„±ë„ í–¥ìƒ
"""

import sys
import json
import re
import time
import random
import threading
from datetime import datetime
from PyQt5.QtWidgets import (QApplication, QMainWindow, QWidget, QVBoxLayout, 
                            QHBoxLayout, QLabel, QLineEdit, QPushButton, 
                            QTextEdit, QProgressBar, QComboBox, QGroupBox,
                            QMessageBox, QTabWidget, QTableWidget, QTableWidgetItem,
                            QCheckBox, QListWidget, QListWidgetItem, QSpinBox,
                            QFileDialog, QSlider, QSplitter)
from PyQt5.QtCore import QThread, pyqtSignal, Qt, QTimer
from PyQt5.QtGui import QFont, QIcon
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from fake_useragent import UserAgent
import queue
import hashlib
import os

class CommercialCrawler:
    """ìƒìš©í™” í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.session = requests.Session()
        self.ua = UserAgent()
        self.proxy_list = []
        self.current_proxy = None
        self.retry_count = 3
        self.delay_range = (1, 3)
        self.max_concurrent = 5
        self.crawled_urls = set()
        self.error_log = []
        
    def load_proxies(self, proxy_file=None):
        """í”„ë¡ì‹œ ëª©ë¡ ë¡œë“œ"""
        if proxy_file and os.path.exists(proxy_file):
            with open(proxy_file, 'r') as f:
                self.proxy_list = [line.strip() for line in f if line.strip()]
        else:
            # ê¸°ë³¸ í”„ë¡ì‹œ ëª©ë¡ (ì‹¤ì œ ì‚¬ìš©ì‹œ ìœ ë£Œ í”„ë¡ì‹œ ì„œë¹„ìŠ¤ ê¶Œì¥)
            self.proxy_list = [
                # ì—¬ê¸°ì— ì‹¤ì œ í”„ë¡ì‹œ ì„œë²„ ì •ë³´ ì…ë ¥
            ]
    
    def get_random_proxy(self):
        """ëœë¤ í”„ë¡ì‹œ ì„ íƒ"""
        if self.proxy_list:
            return random.choice(self.proxy_list)
        return None
    
    def get_headers(self):
        """ëœë¤ í—¤ë” ìƒì„±"""
        return {
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0'
        }
    
    def make_request(self, url, use_proxy=True, timeout=30):
        """ìš”ì²­ ë³´ë‚´ê¸° (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        for attempt in range(self.retry_count):
            try:
                headers = self.get_headers()
                proxies = None
                
                if use_proxy and self.proxy_list:
                    proxy = self.get_random_proxy()
                    if proxy:
                        proxies = {'http': proxy, 'https': proxy}
                
                # ëœë¤ ë”œë ˆì´
                time.sleep(random.uniform(*self.delay_range))
                
                response = self.session.get(
                    url, 
                    headers=headers, 
                    proxies=proxies, 
                    timeout=timeout
                )
                response.raise_for_status()
                return response
                
            except Exception as e:
                self.error_log.append({
                    'url': url,
                    'attempt': attempt + 1,
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                })
                
                if attempt == self.retry_count - 1:
                    raise e
                
                time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
        
        return None

class AdvancedCrawlerThread(QThread):
    """ê³ ê¸‰ í¬ë¡¤ë§ ì‘ì—…ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰"""
    progress_signal = pyqtSignal(str)
    finished_signal = pyqtSignal(dict)
    error_signal = pyqtSignal(str)
    data_signal = pyqtSignal(dict)  # ì‹¤ì‹œê°„ ë°ì´í„° ì „ì†¡
    
    def __init__(self, url, options=None):
        super().__init__()
        self.url = url
        self.options = options or {}
        self.is_running = True
        self.crawler = CommercialCrawler()
        
    def run(self):
        try:
            self.progress_signal.emit("í¬ë¡¤ë§ ì‹œì‘...")
            result = self.advanced_crawl()
            
            if self.is_running:
                self.finished_signal.emit(result)
                
        except Exception as e:
            self.error_signal.emit(str(e))
    
    def advanced_crawl(self):
        """ê³ ê¸‰ í¬ë¡¤ë§"""
        self.progress_signal.emit("í˜ì´ì§€ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        
        # í”„ë¡ì‹œ ì‚¬ìš© ì—¬ë¶€
        use_proxy = self.options.get('use_proxy', False)
        if use_proxy:
            self.crawler.load_proxies(self.options.get('proxy_file'))
        
        # ìš”ì²­ ë³´ë‚´ê¸°
        response = self.crawler.make_request(self.url, use_proxy=use_proxy)
        
        self.progress_signal.emit("í˜ì´ì§€ íŒŒì‹± ì¤‘...")
        
        # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ê¸°ë³¸ ì •ë³´
        data = {
            'url': self.url,
            'timestamp': datetime.now().isoformat(),
            'crawler_info': {
                'user_agent': response.request.headers.get('User-Agent', ''),
                'proxy_used': self.crawler.current_proxy,
                'response_time': response.elapsed.total_seconds(),
                'status_code': response.status_code
            },
            'extracted_data': {},
            'basic_info': {},
            'performance_metrics': {}
        }
        
        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        self.progress_signal.emit("ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ ì¤‘...")
        basic_info = self.extract_basic_info(soup)
        data['basic_info'] = basic_info
        
        # ê³ ê¸‰ ì •ë³´ ì¶”ì¶œ
        self.progress_signal.emit("ê³ ê¸‰ ì •ë³´ ì¶”ì¶œ ì¤‘...")
        advanced_info = self.extract_advanced_info(soup)
        data['extracted_data'] = advanced_info
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        self.progress_signal.emit("ì„±ëŠ¥ ë¶„ì„ ì¤‘...")
        performance = self.calculate_performance_metrics(soup, response)
        data['performance_metrics'] = performance
        
        # ì‹¤ì‹œê°„ ë°ì´í„° ì „ì†¡
        self.data_signal.emit(data)
        
        self.progress_signal.emit("í¬ë¡¤ë§ ì™„ë£Œ!")
        return data
    
    def extract_basic_info(self, soup):
        """ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ"""
        info = {
            'title': '',
            'description': '',
            'keywords': [],
            'images': [],
            'links': [],
            'text_content': '',
            'headers': {},
            'meta_tags': {},
            'forms': [],
            'scripts': [],
            'styles': []
        }
        
        # ì œëª©
        title_tag = soup.find('title')
        if title_tag:
            info['title'] = title_tag.get_text(strip=True)
        
        # ë©”íƒ€ íƒœê·¸
        for meta in soup.find_all('meta'):
            name = meta.get('name', '').lower()
            content = meta.get('content', '')
            
            if name == 'description':
                info['description'] = content
            elif name == 'keywords':
                info['keywords'] = [kw.strip() for kw in content.split(',')]
            else:
                info['meta_tags'][name] = content
        
        # í—¤ë”
        for i in range(1, 7):
            headers = soup.find_all(f'h{i}')
            info['headers'][f'h{i}'] = [h.get_text(strip=True) for h in headers]
        
        # ì´ë¯¸ì§€
        for img in soup.find_all('img', src=True):
            src = img['src']
            alt = img.get('alt', '')
            info['images'].append({
                'src': urljoin(self.url, src),
                'alt': alt,
                'width': img.get('width', ''),
                'height': img.get('height', '')
            })
        
        # ë§í¬
        base_domain = urlparse(self.url).netloc
        for link in soup.find_all('a', href=True):
            href = link['href']
            absolute_url = urljoin(self.url, href)
            link_domain = urlparse(absolute_url).netloc
            
            info['links'].append({
                'url': absolute_url,
                'text': link.get_text(strip=True),
                'title': link.get('title', ''),
                'is_internal': link_domain == base_domain
            })
        
        # í¼
        for form in soup.find_all('form'):
            info['forms'].append({
                'action': form.get('action', ''),
                'method': form.get('method', 'get'),
                'inputs': [{'name': inp.get('name', ''), 'type': inp.get('type', '')} 
                          for inp in form.find_all('input')]
            })
        
        # ìŠ¤í¬ë¦½íŠ¸
        for script in soup.find_all('script'):
            info['scripts'].append({
                'src': script.get('src', ''),
                'type': script.get('type', ''),
                'content_length': len(script.get_text())
            })
        
        # ìŠ¤íƒ€ì¼
        for style in soup.find_all('style'):
            info['styles'].append({
                'content_length': len(style.get_text())
            })
        
        # í…ìŠ¤íŠ¸
        for script in soup(["script", "style", "nav", "footer"]):
            script.decompose()
        
        text_content = soup.get_text(separator=' ', strip=True)
        info['text_content'] = text_content[:2000] + "..." if len(text_content) > 2000 else text_content
        
        return info
    
    def extract_advanced_info(self, soup):
        """ê³ ê¸‰ ì •ë³´ ì¶”ì¶œ"""
        advanced = {
            'content_analysis': {},
            'seo_metrics': {},
            'accessibility': {},
            'security': {}
        }
        
        # ì½˜í…ì¸  ë¶„ì„
        text_content = soup.get_text()
        advanced['content_analysis'] = {
            'word_count': len(text_content.split()),
            'character_count': len(text_content),
            'paragraph_count': len(soup.find_all('p')),
            'sentence_count': len(re.split(r'[.!?]+', text_content)),
            'average_sentence_length': len(text_content.split()) / max(len(re.split(r'[.!?]+', text_content)), 1)
        }
        
        # SEO ë©”íŠ¸ë¦­
        advanced['seo_metrics'] = {
            'title_length': len(soup.find('title').get_text() if soup.find('title') else ''),
            'meta_description_length': len(soup.find('meta', attrs={'name': 'description'}).get('content', '') if soup.find('meta', attrs={'name': 'description'}) else ''),
            'h1_count': len(soup.find_all('h1')),
            'h2_count': len(soup.find_all('h2')),
            'image_count': len(soup.find_all('img')),
            'image_with_alt': len([img for img in soup.find_all('img') if img.get('alt')]),
            'internal_links': len([link for link in soup.find_all('a', href=True) if urlparse(link['href']).netloc == urlparse(self.url).netloc]),
            'external_links': len([link for link in soup.find_all('a', href=True) if urlparse(link['href']).netloc != urlparse(self.url).netloc])
        }
        
        # ì ‘ê·¼ì„±
        advanced['accessibility'] = {
            'images_without_alt': len([img for img in soup.find_all('img') if not img.get('alt')]),
            'forms_without_labels': len([form for form in soup.find_all('form') if not form.find_all('label')]),
            'tables_without_headers': len([table for table in soup.find_all('table') if not table.find_all('th')])
        }
        
        # ë³´ì•ˆ
        advanced['security'] = {
            'has_https': self.url.startswith('https'),
            'has_csp': bool(soup.find('meta', attrs={'http-equiv': 'Content-Security-Policy'})),
            'has_hsts': bool(soup.find('meta', attrs={'http-equiv': 'Strict-Transport-Security'}))
        }
        
        return advanced
    
    def calculate_performance_metrics(self, soup, response):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        return {
            'page_size': len(response.content),
            'html_size': len(response.text),
            'css_size': sum(len(style.get_text()) for style in soup.find_all('style')),
            'js_size': sum(len(script.get_text()) for script in soup.find_all('script')),
            'image_count': len(soup.find_all('img')),
            'script_count': len(soup.find_all('script')),
            'style_count': len(soup.find_all('style')),
            'link_count': len(soup.find_all('a')),
            'form_count': len(soup.find_all('form'))
        }
    
    def stop(self):
        """í¬ë¡¤ë§ ì¤‘ì§€"""
        self.is_running = False

class CommercialWebCrawlerGUI(QMainWindow):
    """ìƒìš©í™” ì›¹ í¬ë¡¤ëŸ¬ GUI"""
    
    def __init__(self):
        super().__init__()
        self.crawler_thread = None
        self.crawler = CommercialCrawler()
        self.init_ui()
        
    def init_ui(self):
        """UI ì´ˆê¸°í™”"""
        self.setWindowTitle("ğŸš€ ìƒìš©í™” ì›¹ í¬ë¡¤ëŸ¬ Pro")
        self.setGeometry(100, 100, 1600, 1000)
        
        # ì¤‘ì•™ ìœ„ì ¯
        central_widget = QWidget()
        self.setCentralWidget(central_widget)
        
        # ë©”ì¸ ë ˆì´ì•„ì›ƒ
        layout = QVBoxLayout(central_widget)
        
        # ì œëª©
        title_label = QLabel("ìƒìš©í™” ì›¹ í¬ë¡¤ëŸ¬ Pro")
        title_label.setFont(QFont("Arial", 18, QFont.Bold))
        title_label.setAlignment(Qt.AlignCenter)
        layout.addWidget(title_label)
        
        # URL ì…ë ¥ ê·¸ë£¹
        url_group = QGroupBox("URL ì…ë ¥")
        url_layout = QHBoxLayout(url_group)
        
        url_label = QLabel("URL:")
        self.url_input = QLineEdit()
        self.url_input.setPlaceholderText("https://example.com")
        
        self.crawl_button = QPushButton("í¬ë¡¤ë§ ì‹œì‘")
        self.crawl_button.clicked.connect(self.start_crawling)
        
        self.stop_button = QPushButton("ì¤‘ì§€")
        self.stop_button.clicked.connect(self.stop_crawling)
        self.stop_button.setEnabled(False)
        
        url_layout.addWidget(url_label)
        url_layout.addWidget(self.url_input)
        url_layout.addWidget(self.crawl_button)
        url_layout.addWidget(self.stop_button)
        
        layout.addWidget(url_group)
        
        # ê³ ê¸‰ ì˜µì…˜ ê·¸ë£¹
        options_group = QGroupBox("ê³ ê¸‰ ì˜µì…˜")
        options_layout = QHBoxLayout(options_group)
        
        # í”„ë¡ì‹œ ì˜µì…˜
        self.use_proxy_checkbox = QCheckBox("í”„ë¡ì‹œ ì‚¬ìš©")
        self.proxy_file_button = QPushButton("í”„ë¡ì‹œ íŒŒì¼ ì„ íƒ")
        self.proxy_file_button.clicked.connect(self.select_proxy_file)
        
        # ë”œë ˆì´ ì„¤ì •
        delay_label = QLabel("ë”œë ˆì´ (ì´ˆ):")
        self.delay_slider = QSlider(Qt.Horizontal)
        self.delay_slider.setRange(1, 10)
        self.delay_slider.setValue(3)
        self.delay_value_label = QLabel("3")
        self.delay_slider.valueChanged.connect(lambda v: self.delay_value_label.setText(str(v)))
        
        # ì¬ì‹œë„ íšŸìˆ˜
        retry_label = QLabel("ì¬ì‹œë„:")
        self.retry_spinbox = QSpinBox()
        self.retry_spinbox.setRange(1, 10)
        self.retry_spinbox.setValue(3)
        
        options_layout.addWidget(self.use_proxy_checkbox)
        options_layout.addWidget(self.proxy_file_button)
        options_layout.addWidget(delay_label)
        options_layout.addWidget(self.delay_slider)
        options_layout.addWidget(self.delay_value_label)
        options_layout.addWidget(retry_label)
        options_layout.addWidget(self.retry_spinbox)
        options_layout.addStretch()
        
        layout.addWidget(options_group)
        
        # ì§„í–‰ë¥  í‘œì‹œ
        self.progress_bar = QProgressBar()
        self.progress_bar.setVisible(False)
        layout.addWidget(self.progress_bar)
        
        # ìƒíƒœ í‘œì‹œ
        self.status_label = QLabel("ëŒ€ê¸° ì¤‘...")
        layout.addWidget(self.status_label)
        
        # íƒ­ ìœ„ì ¯
        self.tab_widget = QTabWidget()
        layout.addWidget(self.tab_widget)
        
        # ê²°ê³¼ íƒ­
        self.result_text = QTextEdit()
        self.result_text.setReadOnly(True)
        self.tab_widget.addTab(self.result_text, "ì „ì²´ ê²°ê³¼")
        
        # ì„±ëŠ¥ ë¶„ì„ íƒ­
        self.performance_text = QTextEdit()
        self.performance_text.setReadOnly(True)
        self.tab_widget.addTab(self.performance_text, "ì„±ëŠ¥ ë¶„ì„")
        
        # SEO ë¶„ì„ íƒ­
        self.seo_text = QTextEdit()
        self.seo_text.setReadOnly(True)
        self.tab_widget.addTab(self.seo_text, "SEO ë¶„ì„")
        
        # ì—ëŸ¬ ë¡œê·¸ íƒ­
        self.error_text = QTextEdit()
        self.error_text.setReadOnly(True)
        self.tab_widget.addTab(self.error_text, "ì—ëŸ¬ ë¡œê·¸")
        
        # ìš”ì•½ íƒ­
        self.summary_table = QTableWidget()
        self.tab_widget.addTab(self.summary_table, "ìš”ì•½ ì •ë³´")
        
        # ë²„íŠ¼ ê·¸ë£¹
        button_layout = QHBoxLayout()
        
        self.save_button = QPushButton("ê²°ê³¼ ì €ì¥")
        self.save_button.clicked.connect(self.save_results)
        self.save_button.setEnabled(False)
        
        self.export_excel_button = QPushButton("Excel ë‚´ë³´ë‚´ê¸°")
        self.export_excel_button.clicked.connect(self.export_to_excel)
        self.export_excel_button.setEnabled(False)
        
        self.clear_button = QPushButton("ì´ˆê¸°í™”")
        self.clear_button.clicked.connect(self.clear_results)
        
        button_layout.addWidget(self.save_button)
        button_layout.addWidget(self.export_excel_button)
        button_layout.addWidget(self.clear_button)
        button_layout.addStretch()
        
        layout.addLayout(button_layout)
        
        # ê²°ê³¼ ë°ì´í„° ì €ì¥
        self.crawl_result = None
        self.proxy_file = None
        
    def select_proxy_file(self):
        """í”„ë¡ì‹œ íŒŒì¼ ì„ íƒ"""
        file_path, _ = QFileDialog.getOpenFileName(
            self, "í”„ë¡ì‹œ íŒŒì¼ ì„ íƒ", "", "Text Files (*.txt);;All Files (*)"
        )
        if file_path:
            self.proxy_file = file_path
            QMessageBox.information(self, "ì„±ê³µ", f"í”„ë¡ì‹œ íŒŒì¼ì´ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤: {file_path}")
        
    def start_crawling(self):
        """í¬ë¡¤ë§ ì‹œì‘"""
        url = self.url_input.text().strip()
        
        if not url:
            QMessageBox.warning(self, "ê²½ê³ ", "URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
            self.url_input.setText(url)
        
        # UI ìƒíƒœ ë³€ê²½
        self.crawl_button.setEnabled(False)
        self.stop_button.setEnabled(True)
        self.progress_bar.setVisible(True)
        self.progress_bar.setRange(0, 0)
        self.status_label.setText("í¬ë¡¤ë§ ì¤€ë¹„ ì¤‘...")
        
        # ì˜µì…˜ ì„¤ì •
        options = {
            'use_proxy': self.use_proxy_checkbox.isChecked(),
            'proxy_file': self.proxy_file,
            'delay_range': (self.delay_slider.value(), self.delay_slider.value() + 1),
            'retry_count': self.retry_spinbox.value()
        }
        
        # í¬ë¡¤ë§ ìŠ¤ë ˆë“œ ì‹œì‘
        self.crawler_thread = AdvancedCrawlerThread(url, options)
        self.crawler_thread.progress_signal.connect(self.update_progress)
        self.crawler_thread.finished_signal.connect(self.crawling_finished)
        self.crawler_thread.error_signal.connect(self.crawling_error)
        self.crawler_thread.data_signal.connect(self.update_data)
        self.crawler_thread.start()
        
    def stop_crawling(self):
        """í¬ë¡¤ë§ ì¤‘ì§€"""
        if self.crawler_thread:
            self.crawler_thread.stop()
            self.crawler_thread.wait()
        
        self.crawl_button.setEnabled(True)
        self.stop_button.setEnabled(False)
        self.progress_bar.setVisible(False)
        self.status_label.setText("í¬ë¡¤ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    def update_progress(self, message):
        """ì§„í–‰ë¥  ì—…ë°ì´íŠ¸"""
        self.status_label.setText(message)
        
    def update_data(self, data):
        """ì‹¤ì‹œê°„ ë°ì´í„° ì—…ë°ì´íŠ¸"""
        self.crawl_result = data
        self.display_results(data)
        
    def crawling_finished(self, result):
        """í¬ë¡¤ë§ ì™„ë£Œ"""
        self.crawl_result = result
        
        # UI ìƒíƒœ ë³µì›
        self.crawl_button.setEnabled(True)
        self.stop_button.setEnabled(False)
        self.progress_bar.setVisible(False)
        self.status_label.setText("í¬ë¡¤ë§ ì™„ë£Œ!")
        
        # ê²°ê³¼ í‘œì‹œ
        self.display_results(result)
        
        # ì €ì¥ ë²„íŠ¼ í™œì„±í™”
        self.save_button.setEnabled(True)
        self.export_excel_button.setEnabled(True)
        
    def crawling_error(self, error_message):
        """í¬ë¡¤ë§ ì—ëŸ¬"""
        self.crawl_button.setEnabled(True)
        self.stop_button.setEnabled(False)
        self.progress_bar.setVisible(False)
        self.status_label.setText("í¬ë¡¤ë§ ì‹¤íŒ¨!")
        
        QMessageBox.critical(self, "ì—ëŸ¬", f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{error_message}")
        
    def display_results(self, result):
        """ê²°ê³¼ í‘œì‹œ"""
        # ì „ì²´ ê²°ê³¼ í‘œì‹œ
        self.result_text.setText(json.dumps(result, ensure_ascii=False, indent=2))
        
        # ì„±ëŠ¥ ë¶„ì„ í‘œì‹œ
        performance = result.get('performance_metrics', {})
        self.performance_text.setText(json.dumps(performance, ensure_ascii=False, indent=2))
        
        # SEO ë¶„ì„ í‘œì‹œ
        seo_metrics = result.get('extracted_data', {}).get('seo_metrics', {})
        self.seo_text.setText(json.dumps(seo_metrics, ensure_ascii=False, indent=2))
        
        # ì—ëŸ¬ ë¡œê·¸ í‘œì‹œ
        if hasattr(self.crawler_thread.crawler, 'error_log'):
            self.error_text.setText(json.dumps(self.crawler_thread.crawler.error_log, ensure_ascii=False, indent=2))
        
        # ìš”ì•½ ì •ë³´ í‘œì‹œ
        self.display_summary(result)
        
    def display_summary(self, result):
        """ìš”ì•½ ì •ë³´ í‘œì‹œ"""
        self.summary_table.setRowCount(15)
        self.summary_table.setColumnCount(2)
        self.summary_table.setHorizontalHeaderLabels(["í•­ëª©", "ê°’"])
        
        basic_info = result.get('basic_info', {})
        extracted_data = result.get('extracted_data', {})
        performance = result.get('performance_metrics', {})
        crawler_info = result.get('crawler_info', {})
        
        summary_data = [
            ("URL", result.get('url', '')),
            ("í¬ë¡¤ë§ ì‹œê°„", result.get('timestamp', '')),
            ("ì œëª©", basic_info.get('title', '')),
            ("í˜ì´ì§€ í¬ê¸°", f"{performance.get('page_size', 0):,} bytes"),
            ("ì´ë¯¸ì§€ ìˆ˜", str(len(basic_info.get('images', [])))),
            ("ë§í¬ ìˆ˜", str(len(basic_info.get('links', [])))),
            ("ë‹¨ì–´ ìˆ˜", str(extracted_data.get('content_analysis', {}).get('word_count', 0))),
            ("H1 íƒœê·¸ ìˆ˜", str(extracted_data.get('seo_metrics', {}).get('h1_count', 0))),
            ("ë‚´ë¶€ ë§í¬", str(extracted_data.get('seo_metrics', {}).get('internal_links', 0))),
            ("ì™¸ë¶€ ë§í¬", str(extracted_data.get('seo_metrics', {}).get('external_links', 0))),
            ("User-Agent", crawler_info.get('user_agent', '')[:50] + "..."),
            ("í”„ë¡ì‹œ ì‚¬ìš©", "ì˜ˆ" if crawler_info.get('proxy_used') else "ì•„ë‹ˆì˜¤"),
            ("ì‘ë‹µ ì‹œê°„", f"{crawler_info.get('response_time', 0):.2f}ì´ˆ"),
            ("ìƒíƒœ ì½”ë“œ", str(crawler_info.get('status_code', ''))),
            ("HTTPS ì‚¬ìš©", "ì˜ˆ" if extracted_data.get('security', {}).get('has_https') else "ì•„ë‹ˆì˜¤")
        ]
        
        for i, (key, value) in enumerate(summary_data):
            self.summary_table.setItem(i, 0, QTableWidgetItem(key))
            self.summary_table.setItem(i, 1, QTableWidgetItem(value))
        
        self.summary_table.resizeColumnsToContents()
        
    def save_results(self):
        """ê²°ê³¼ ì €ì¥"""
        if not self.crawl_result:
            return
            
        filename = f"commercial_crawl_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.crawl_result, f, ensure_ascii=False, indent=2)
            
            QMessageBox.information(self, "ì„±ê³µ", f"ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            QMessageBox.critical(self, "ì—ëŸ¬", f"ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{str(e)}")
    
    def export_to_excel(self):
        """Excelë¡œ ë‚´ë³´ë‚´ê¸°"""
        if not self.crawl_result:
            return
            
        try:
            import pandas as pd
            
            # ë°ì´í„° ì¤€ë¹„
            data = []
            basic_info = self.crawl_result.get('basic_info', {})
            extracted_data = self.crawl_result.get('extracted_data', {})
            
            # ê¸°ë³¸ ì •ë³´
            data.append(['URL', self.crawl_result.get('url', '')])
            data.append(['ì œëª©', basic_info.get('title', '')])
            data.append(['ì„¤ëª…', basic_info.get('description', '')])
            data.append(['ë‹¨ì–´ ìˆ˜', extracted_data.get('content_analysis', {}).get('word_count', 0)])
            data.append(['ì´ë¯¸ì§€ ìˆ˜', len(basic_info.get('images', []))])
            data.append(['ë§í¬ ìˆ˜', len(basic_info.get('links', []))])
            
            # DataFrame ìƒì„±
            df = pd.DataFrame(data, columns=['í•­ëª©', 'ê°’'])
            
            # íŒŒì¼ ì €ì¥
            filename = f"crawl_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
            df.to_excel(filename, index=False)
            
            QMessageBox.information(self, "ì„±ê³µ", f"Excel íŒŒì¼ì´ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except ImportError:
            QMessageBox.warning(self, "ê²½ê³ ", "Excel ë‚´ë³´ë‚´ê¸°ë¥¼ ìœ„í•´ pandasë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install pandas openpyxl")
        except Exception as e:
            QMessageBox.critical(self, "ì—ëŸ¬", f"Excel ë‚´ë³´ë‚´ê¸° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{str(e)}")
            
    def clear_results(self):
        """ê²°ê³¼ ì´ˆê¸°í™”"""
        self.result_text.clear()
        self.performance_text.clear()
        self.seo_text.clear()
        self.error_text.clear()
        self.summary_table.setRowCount(0)
        self.crawl_result = None
        self.save_button.setEnabled(False)
        self.export_excel_button.setEnabled(False)
        self.status_label.setText("ëŒ€ê¸° ì¤‘...")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    app = QApplication(sys.argv)
    app.setStyle('Fusion')
    
    window = CommercialWebCrawlerGUI()
    window.show()
    
    sys.exit(app.exec_())

if __name__ == '__main__':
    main()
